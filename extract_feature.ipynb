{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6p6yVEJ5cbR"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import librosa.display\n",
        "import IPython.display as ipd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import librosa\n",
        "import librosa.display\n",
        "import IPython.display as ipd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import soundfile as sf\n",
        "import os\n",
        "import glob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yrdn5Gc7uiJ"
      },
      "source": [
        "## tạo file csv cho cơ sở dữ liệu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6Fj4qDL8go-"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "tạo file csv cơ sở dữ liệu cho mô hình wakeword\n"
      ],
      "metadata": {
        "id": "T915DF9r93Dp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_csv(folder, label):\n",
        "    folder_name = os.path.basename(folder)\n",
        "    labels = list(label)\n",
        "    print(labels)\n",
        "    # generate CSV file\n",
        "    df = pd.DataFrame(columns=[\"filepath\",\"label\"])\n",
        "    i = 0\n",
        "    for label in labels:\n",
        "      print(\"Reading\",os.path.join(folder,label, \"*\"))\n",
        "      for filepath in glob.glob(os.path.join(folder,label, \"*.wav\")):\n",
        "            df.loc[i] = [filepath,label]\n",
        "            i += 1\n",
        "      output_file = f\"{folder_name}.csv\"\n",
        "    print(\"Saving\", output_file)\n",
        "    df.to_csv(output_file)\n",
        "dir_folder = '/content/drive/MyDrive/wakeword'\n",
        "generate_csv(dir_folder ,{'lisa oi','noise'})\n",
        "metadata_filename = \"wakeword.csv\"\n",
        "# load CSV files as DataFrames\n",
        "df0 = pd.read_csv(metadata_filename)\n",
        "df0 = df0.sample(frac=1).reset_index(drop=True)\n",
        "df0.tail()"
      ],
      "metadata": {
        "id": "Q8lx1wvgoFc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "tạo file csv cơ sở dữ liệu cho mô hình nhận dạng câu lệnh"
      ],
      "metadata": {
        "id": "wYjHnvkC-C4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_csv(folder, gender, label):\n",
        "    folder_name = os.path.basename(folder)\n",
        "    labels = list(label)\n",
        "    genders = list(gender)\n",
        "    print(labels)\n",
        "    # generate CSV file\n",
        "    df = pd.DataFrame(columns=[\"filepath\",\"label\"])\n",
        "    i = 0\n",
        "    for label in labels:\n",
        "      print(\"Reading\",os.path.join(folder,label, \"*\"))\n",
        "      for gender in genders:\n",
        "          for filepath in glob.glob(os.path.join(folder,label, gender, \"*.wav\")):\n",
        "            df.loc[i] = [filepath,label]\n",
        "            i += 1\n",
        "      output_file = f\"{folder_name}.csv\"\n",
        "    print(\"Saving\", output_file)\n",
        "    df.to_csv(output_file)\n",
        "dir_folder = '/content/drive/MyDrive/aug_dataset'\n",
        "generate_csv(dir_folder,{'nu','nam'} ,{'den bat','den tat','den tang sang','den giam sang','den chuyen mau','noise'})\n",
        "metadata_filename = \"command.csv\"\n",
        "# load CSV files as DataFrames\n",
        "df1 = pd.read_csv(metadata_filename)\n",
        "df1 = df1.sample(frac=1).reset_index(drop=True)\n",
        "df1.tail(3)"
      ],
      "metadata": {
        "id": "1a8QSJhZoLgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-ehR7-zu-KYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# tạo hàm trích xuất đặc trưng MFCC"
      ],
      "metadata": {
        "id": "ZMb_AAAi-L5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_transform(file):\n",
        "    audio,rate = librosa.load(file,sr=16000)\n",
        "    mfccs = librosa.feature.mfcc(y=audio,n_mfcc=13,sr=16000)\n",
        "    delta_mfccs = librosa.feature.delta(mfccs)\n",
        "    delta2_mfccs = librosa.feature.delta(mfccs, order=2)\n",
        "    features = np.concatenate((mfccs, delta_mfccs, delta2_mfccs))\n",
        "    features = features.transpose()\n",
        "    means = np.mean(features, axis=1, keepdims=True)\n",
        "    stddevs = np.std(features, axis=1, keepdims=True)\n",
        "    mfcc_feature = (features - means) / (stddevs + 1e-10)\n",
        "    mfccs_feature = np.zeros((34,39))\n",
        "    mfccs_feature[:mfcc_feature.shape[0], :mfcc_feature.shape[1]]=mfcc_feature\n",
        "    return mfccs_feature"
      ],
      "metadata": {
        "id": "EZvDDP64rVcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trích xuất đặc trưng và lưu các đặc trưng"
      ],
      "metadata": {
        "id": "DjAKSaSW-VdF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.contrib.concurrent import process_map\n",
        "feature_mfcc_wakeword = process_map(get_transform, df0['filepath'],max_workers=8)"
      ],
      "metadata": {
        "id": "EdhoOs03yXw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_mfcc_command = process_map(get_transform, df1['filepath'],max_workers=8)"
      ],
      "metadata": {
        "id": "h-E9cbBb_YPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def label_command_encode(label):\n",
        "  if label == 'den bat':\n",
        "    return 0\n",
        "  elif label == 'den tat':\n",
        "    return 1\n",
        "  elif label == 'den tang sang':\n",
        "    return 2\n",
        "  elif label == 'den giam sang':\n",
        "    return 3\n",
        "  elif label == 'den chuyen mau':\n",
        "    return 4\n",
        "  elif label == 'noise':\n",
        "    return 5\n",
        "\n",
        "def label_wakeword_encode(label):\n",
        "  if label == 'lisa oi':\n",
        "    return 1\n",
        "  elif label == 'noise':\n",
        "    return 0"
      ],
      "metadata": {
        "id": "XUJdYIGS6rkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df0['label_en'] = df0['label'].apply(label_wakeword_encode)\n",
        "df1['label_en'] = df1['label'].apply(label_command_encode)"
      ],
      "metadata": {
        "id": "OD6UWH-H6-QV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "split_train_0 = int(len(df0['label_en']) * 0.8)\n",
        "split_test_0 = int(len(df0['label_en']) * 0.9)\n",
        "X_train_wakeword = feature_mfcc_wakeword[:split_train_0]\n",
        "X_val_wakeword = feature_mfcc_wakeword[split_train_0:split_test_0]\n",
        "X_test_wakeword = feature_mfcc_wakeword[split_test_0:]\n",
        "y_train_wakeword = np.array(df0['label_en'][:split_train_0].tolist())\n",
        "y_val_wakeword = np.array(df0['label_en'][split_train_0:split_test_0].tolist())\n",
        "y_test_wakeword = np.array(df0['label_en'][split_test_0:].tolist())\n",
        "\n",
        "print(f\"Size of the training set: {len(X_train_wakeword)}\")\n",
        "print(f\"Size of the test,val set: {len(X_val_wakeword)}\")\n",
        "print(f\"Size of the test,val set: {len(X_test_wakeword)}\")"
      ],
      "metadata": {
        "id": "bclr_h1p7qZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_wakeword_data = np.stack(X_train_wakeword)\n",
        "X_val_wakeword_data = np.stack(X_val_wakeword)\n",
        "X_test_wakeword_data = np.stack(X_test_wakeword)\n",
        "y_test_wakeword_data =np.array(y_test_wakeword)\n",
        "y_train_wakeword_data =np.array(y_train_wakeword)\n",
        "y_val_wakeword_data =np.array(y_val_wakeword)"
      ],
      "metadata": {
        "id": "kf7WNfae8r81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_sets_file0 = '/content/drive/MyDrive/wakeword/wakeword_feature.npz'\n",
        "np.savez(feature_sets_file0,\n",
        "         x_train=X_train_wakeword_data,\n",
        "         y_train=y_train_wakeword_data,\n",
        "         x_val=X_val_wakeword_data,\n",
        "         y_val=y_val_wakeword_data,\n",
        "         x_test=X_test_wakeword_data,\n",
        "         y_test=y_test_wakeword_data)"
      ],
      "metadata": {
        "id": "4oGjHg568uV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_train_1 = int(len(df1['label_en']) * 0.8)\n",
        "split_test_1 = int(len(df1['label_en']) * 0.9)\n",
        "X_train_command = feature_mfcc_command[:split_train_1]\n",
        "X_val_command = feature_mfcc_command[split_train_1:split_test_1]\n",
        "X_test_command = feature_mfcc_command[split_test_1:]\n",
        "y_train_command = to_categorical(np.array(df1['label_en'][:split_train_1].tolist()))\n",
        "y_val_command = to_categorical(np.array(df1['label_en'][split_train_1:split_test_1].tolist()))\n",
        "y_test_command = to_categorical(np.array(df1['label_en'][split_test_1:].tolist()))\n",
        "\n",
        "print(f\"Size of the training set: {len(X_train_command)}\")\n",
        "print(f\"Size of the test,val set: {len(X_val_command)}\")\n",
        "print(f\"Size of the test,val set: {len(X_test_command)}\")"
      ],
      "metadata": {
        "id": "XDPmrHy4DHDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_command_data = np.stack(X_train_command)\n",
        "X_val_command_data = np.stack(X_val_command)\n",
        "X_test_command_data = np.stack(X_test_command)\n",
        "y_test_command_data =np.array(y_test_command)\n",
        "y_train_command_data =np.array(y_train_command)\n",
        "y_val_command_data =np.array(y_val_command)"
      ],
      "metadata": {
        "id": "b1hXX0a2DU5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_sets_file1 = '/content/drive/MyDrive/wakeword/command_feature.npz'\n",
        "np.savez(feature_sets_file1,\n",
        "         x_train=X_train_command_data,\n",
        "         y_train=y_train_command_data,\n",
        "         x_val=X_val_command_data,\n",
        "         y_val=y_val_command_data,\n",
        "         x_test=X_test_command_data,\n",
        "         y_test=y_test_command_data)"
      ],
      "metadata": {
        "id": "9y-bucFM7mWp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}